{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f435810",
   "metadata": {},
   "source": [
    "# 거리를 나타내는 컬럼만 사용하여 선형 회귀 모델을 만들어본다.\n",
    "## 인스턴스 생성 시 부터 맥스 메모리(Max Memory)를 지정해준다.\n",
    "## Out of Memory 증상을 미연에 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb004440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 17:00:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 인스턴스 생성(Max Memory 지정: Out of Memory 방지)\n",
    "MAX_MEMORY=\"5g\"\n",
    "spark = SparkSession.builder.appName(\"taxi-fare-prediciton\").config(\"spark.executor.memory\", MAX_MEMORY).config(\"spark.driver.memory\", MAX_MEMORY).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b4e9638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 파일 불러오기\n",
    "trips_df = spark.read.csv('/home/lab01/src/data/trip/*', inferSchema=True, header=True)\n",
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023a8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "637d0444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|trip_distance|total_amount|\n",
      "+-------------+------------+\n",
      "|         16.5|       70.07|\n",
      "|         1.13|       11.16|\n",
      "|         2.68|       18.59|\n",
      "|         12.4|        43.8|\n",
      "|          9.7|        32.3|\n",
      "|          9.3|       43.67|\n",
      "|         9.58|        46.1|\n",
      "|         16.2|        45.3|\n",
      "|         3.58|        19.3|\n",
      "|         0.91|        14.8|\n",
      "|         2.57|        12.8|\n",
      "|          0.4|         5.3|\n",
      "|         3.26|        17.3|\n",
      "|        13.41|       47.25|\n",
      "|         18.3|       61.42|\n",
      "|         1.53|       14.16|\n",
      "|          2.0|        11.8|\n",
      "|         16.6|       54.96|\n",
      "|         15.5|       56.25|\n",
      "|          1.3|        16.8|\n",
      "+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL_analysis.ipynb에서 진행했던 이상치 제거를 적용한다.\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    trip_distance,\n",
    "    total_amount\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 5\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "    AND TO_DATE(tpep_pickup_datetime) < '2021-08-01'\n",
    "\"\"\"\n",
    "\n",
    "data_df = spark.sql(query)\n",
    "data_df.createOrReplaceTempView(\"data\")\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347f2475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|     trip_distance|      total_amount|\n",
      "+-------+------------------+------------------+\n",
      "|  count|          13326131|          13326131|\n",
      "|   mean|2.8874228408830245|17.990713821025437|\n",
      "| stddev|3.8336178303381048| 13.01169363051562|\n",
      "|    min|              0.01|              0.01|\n",
      "|    max|             475.5|            4973.3|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8237cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10660955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2665176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train / test split\n",
    "train_df, test_df = data_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(train_df.count())\n",
    "print(test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a3849",
   "metadata": {},
   "source": [
    "# train : 10659921 / test : 2666210"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ffb5a",
   "metadata": {},
   "source": [
    "# VectorAssembler\n",
    "### - VectorAssembler는 trip_distance 컬럼을 특징 벡터로 변환합니다. 이를 통해 trip_distance 값을 features라는 새로운 컬럼에 벡터 형태로 저장하게 됩니다.\n",
    "### - features 컬럼은 모델에 입력될 특성 벡터를 나타내며, 이 값은 total_amount(목표 변수)를 예측하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078866bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------+\n",
      "|trip_distance|total_amount|features|\n",
      "+-------------+------------+--------+\n",
      "|         0.01|        3.05|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "+-------------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vassembler = VectorAssembler(inputCols=[\"trip_distance\"], outputCol=\"features\")\n",
    "vtrain_df = vassembler.transform(train_df)\n",
    "vtrain_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d12ccb",
   "metadata": {},
   "source": [
    "# LinearRegression\n",
    "### - 선형 회귀 모델을 사용하여 trip_distance(이동 거리)를 입력으로 total_amount(총 금액)를 예측하는 모델을 생성합니다.\n",
    "### - prediction 컬럼은 trip_distance에 대한 예측된 total_amount 값을 보여줍니다. 예를 들어, trip_distance가 0.01일 때 모델이 예측한 total_amount는 9.41입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23fdba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 17:03:37 WARN Instrumentation: [717df371] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/12/18 17:03:45 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/12/18 17:03:45 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "24/12/18 17:04:33 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "24/12/18 17:04:33 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------+-----------------+\n",
      "|trip_distance|total_amount|features|       prediction|\n",
      "+-------------+------------+--------+-----------------+\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.3|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.8|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.8|  [0.01]|9.418361874707077|\n",
      "|         0.01|         3.8|  [0.01]|9.418361874707077|\n",
      "|         0.01|         4.3|  [0.01]|9.418361874707077|\n",
      "+-------------+------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(\n",
    "    maxIter=50, # 모델 학습할 때 최대 50번의 반복을 허용한다.\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "# 선형 회귀 모델 학습\n",
    "model = lr.fit(vtrain_df)\n",
    "vtest_df = vassembler.transform(test_df)\n",
    "\n",
    "# 예측\n",
    "prediction = model.transform(vtest_df)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67981af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.308435272654453"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "# RMSE : 오차를 의미하고 값이 작을 수록 예측 성능이 좋다\n",
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec11b17",
   "metadata": {},
   "source": [
    "## 오차가 약 6.307임을 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6216727b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.765849929974943"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R-squared(결정계수) : 0과 1 사이의 값을 가지며 1에 가까울수록 모델이 데이터를 잘 설명한다는 의미를 가진다.\n",
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c6df8",
   "metadata": {},
   "source": [
    "## 이 모델의 예측 성능은   약 76.57%이다. 그렇게 높게 측정되진 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182fd7ff",
   "metadata": {},
   "source": [
    "# 실 서비스 task\n",
    "실 서비스에서 사용될 모델을 입력 데이터 벡터화 및 예측 결과 출력을 통해 테스트하고 준비하는 과정입니다. trip_distance를 기반으로 total_amount를 예측하는 모델을 실시간으로 적용할 수 있도록 준비하는 작업으로, 실제 서비스에서 유용하게 활용될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c189e8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|trip_distance|\n",
      "+-------------+\n",
      "|          1.1|\n",
      "|          5.5|\n",
      "|         10.5|\n",
      "|         30.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "distance_list = [1.1, 5.5, 10.5, 30.0]\n",
    "distance_df = spark.createDataFrame(distance_list, DoubleType()).toDF(\"trip_distance\")\n",
    "distance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61d254c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------------------+\n",
      "|trip_distance|features|        prediction|\n",
      "+-------------+--------+------------------+\n",
      "|          1.1|   [1.1]|12.666016967333638|\n",
      "|          5.5|   [5.5]|25.775817341239016|\n",
      "|         10.5|  [10.5]| 40.67331776613149|\n",
      "|         30.0|  [30.0]| 98.77356942321214|\n",
      "+-------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vdistance_df = vassembler.transform(distance_df)\n",
    "model.transform(vdistance_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9df8e468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측에 쓰일 컬럼을 추가하고 전처리하여 성능을 올린다.\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as pickup_time,\n",
    "    DATE_FORMAT(TO_DATE(tpep_pickup_datetime), 'EEEE') AS day_of_week,\n",
    "    total_amount\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 5\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "    AND TO_DATE(tpep_pickup_datetime) < '2021-08-01'\n",
    "\"\"\"\n",
    "data_df = spark.sql(query)\n",
    "data_df.createOrReplaceTempView(\"data\")\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb4c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_df.randomSplit([0.8, 0.2], seed=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26656ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나뉘어진 데이터 프레임을 저장해둔다.\n",
    "data_dir = '/home/lab01/src/project/taxi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75134f68",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/lab01/src/project/taxi/train already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 파퀫 형태로 저장\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 압축률이 좋고 디스크 io가 적다\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 컬럼 기반 포맷이다.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 파퀫 형태로 불러오기\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1109\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/lab01/src/project/taxi/train already exists."
     ]
    }
   ],
   "source": [
    "# 파퀫 형태로 저장\n",
    "# 압축률이 좋고 디스크 io가 적다\n",
    "# 컬럼 기반 포맷이다.\n",
    "train_df.write.format(\"parquet\").save(f\"{data_dir}/train/\")\n",
    "test_df.write.format(\"parquet\").save(f\"{data_dir}/test/\")\n",
    "\n",
    "\n",
    "# 파퀫 형태로 불러오기\n",
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df = spark.read.parquet(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14d178",
   "metadata": {},
   "source": [
    "# 스테이지 파이프 라인\n",
    "### 프리프로세싱을 위한 파이프 라인을 생성하여 전처리한다.\n",
    "- 데이터 전처리를 위한 여러 단계를 파이프라인으로 연결하여 훈련 데이터(train_df)에 적용하는 과정입니다. 각 단계는 모델 학습을 위한 데이터 준비를 돕고, 범주형 변수와 수치형 변수를 각각 적절히 변환하여 예측 모델에 사용할 수 있는 형식으로 만듭니다.\n",
    "- 범주형 데이터: pickup_location_id, dropoff_location_id, day_of_week와 같은 범주형 데이터를 숫자형 데이터로 변환하고, 이를 **원핫 인코딩(One-Hot Encoding)**을 통해 모델에 맞는 형식으로 변환합니다.\n",
    "- 수치형 데이터: passenger_count, trip_distance, pickup_time과 같은 수치형 데이터를 **스케일링(Scaling)**하여 모델 학습에 적합하게 만듭니다.\n",
    "- 이 모든 전처리 과정을 하나의 **파이프라인(Pipeline)**으로 연결하여 훈련 데이터에 일괄 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8498a9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_688d07d16fc3,\n",
       " OneHotEncoder_47763915ec36,\n",
       " StringIndexer_4d164e8b3874,\n",
       " OneHotEncoder_48d173cf8562,\n",
       " StringIndexer_daaae5cad3c8,\n",
       " OneHotEncoder_472871429db9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스트링 값을 숫자값으로 바꾸어 원핫인코딩을 진행한다.\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# category feature 설정\n",
    "cat_feats = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "stages = []\n",
    "\n",
    "for c in cat_feats:\n",
    "    # StringIndexer는 범주형 변수(pickup_location_id, dropoff_location_id, day_of_week)를 숫자형 값으로 변환합니다.\n",
    "    # 예를 들어, pickup_location_id가 \"A\", \"B\", \"C\"와 같은 카테고리 값을 갖는 경우, StringIndexer는 이를 0, 1, 2와 같은 숫자로 변환합니다.\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol= c + \"_idx\").setHandleInvalid(\"keep\")\n",
    "    # OneHotEncoder는 StringIndexer로 숫자형으로 변환된 범주형 데이터를 원핫 인코딩 방식으로 변환합니다.\n",
    "    # 예를 들어, pickup_location_id가 0, 1, 2로 변환되었다면, OneHotEncoder는 이를 [1, 0, 0], [0, 1, 0], [0, 0, 1]로 변환합니다.\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c + \"_onehot\"])\n",
    "    stages += [cat_indexer, onehot_encoder]\n",
    "    \n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db0859fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_688d07d16fc3,\n",
       " OneHotEncoder_47763915ec36,\n",
       " StringIndexer_4d164e8b3874,\n",
       " OneHotEncoder_48d173cf8562,\n",
       " StringIndexer_daaae5cad3c8,\n",
       " OneHotEncoder_472871429db9,\n",
       " VectorAssembler_7973979a1d5f,\n",
       " StandardScaler_38efdaadfd72,\n",
       " VectorAssembler_1c2eef4b74a5,\n",
       " StandardScaler_e6408ff8628f,\n",
       " VectorAssembler_a7c09345fc04,\n",
       " StandardScaler_f4a06ee1e2f7]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 수치형 전처리: VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_feats = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_feats:\n",
    "    # VectorAssembler는 단일 수치형 컬럼을 벡터 형식으로 변환\n",
    "    # trip_distance 값이 10.0이라면 이 값은 [10.0]과 같은 벡터로 변환됩니다.\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol= n + \"_vecotr\")\n",
    "    # StandardScaler는 수치형 데이터를 스케일링하여, 평균이 0이고 표준편차가 1인 값으로 변환\n",
    "    # trip_distance 값이 매우 클 경우, 이를 스케일링하지 않으면 모델이 다른 특성에 비해 지나치게 영향을 받을 수 있다\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol= n + \"_scaled\")\n",
    "    stages += [num_assembler, num_scaler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3db832e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_688d07d16fc3,\n",
       " OneHotEncoder_47763915ec36,\n",
       " StringIndexer_4d164e8b3874,\n",
       " OneHotEncoder_48d173cf8562,\n",
       " StringIndexer_daaae5cad3c8,\n",
       " OneHotEncoder_472871429db9,\n",
       " VectorAssembler_7973979a1d5f,\n",
       " StandardScaler_38efdaadfd72,\n",
       " VectorAssembler_1c2eef4b74a5,\n",
       " StandardScaler_e6408ff8628f,\n",
       " VectorAssembler_a7c09345fc04,\n",
       " StandardScaler_f4a06ee1e2f7,\n",
       " VectorAssembler_3faab45ec0f0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 가지를 프리프로세싱을 했는데 합치는 과정을 진행한다.\n",
    "# 데이터의 원핫 인코딩된 값과 스케일링된 수치형 데이터를 하나의 특성 벡터(feature_vector)로 결합하는 과정\n",
    "# assembler_inputs에는 원핫 인코딩된 범주형 특성과 스케일링된 수치형 특성들이 포함\n",
    "assembler_inputs = [c + \"_onehot\" for c in cat_feats] + [n + \"_scaled\" for n in num_feats]\n",
    "# VectorAssembler는 이 모든 데이터를 하나의 특성 벡터(feature_vector)로 결합\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"feature_vector\")\n",
    "stages += [assembler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5b1fa",
   "metadata": {},
   "source": [
    "### - Pipeline: 위에서 정의한 전처리 단계를 파이프라인으로 묶습니다. 파이프라인을 사용하면 여러 단계의 변환을 일괄적으로 처리할 수 있습니다.\n",
    "### - pipeline.fit(train_df)는 훈련 데이터(train_df)에 대해 전처리 과정을 학습하는 부분입니다. 이 과정에서 각 변환 단계들이 실제 데이터에 맞춰 적합(fit)됩니다.\n",
    "### - fitted_transformer.transform(train_df)는 학습된 전처리 파이프라인을 train_df에 적용하여 변환된 데이터(vtrain_df)를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16853b33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 스테이지로 파이프 라인 생성\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transform_stages = stages\n",
    "pipeline = Pipeline(stages=transform_stages)\n",
    "fitted_transformer = pipeline.fit(train_df)\n",
    "\n",
    "\n",
    "# 적용\n",
    "vtrain_df = fitted_transformer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7b7fbd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 33:>                                                        (0 + 2) / 11]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39mstages)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 파이프라인 학습\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 적용\u001b[39;00m\n\u001b[1;32m     51\u001b[0m vtrain_df \u001b[38;5;241m=\u001b[39m fitted_transformer\u001b[38;5;241m.\u001b[39mtransform(train_df)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/pipeline.py:114\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m/home/ubuntu/anaconda3/envs/spark/lib/python3.8/socket.py:681\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # 스트링 값을 숫자값으로 바꾸어 원핫인코딩을 진행한다.\n",
    "# from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# # 카테고리 피쳐 설정\n",
    "# cat_feats = [\n",
    "#     \"pickup_location_id\",\n",
    "#     \"dropoff_location_id\",\n",
    "#     \"day_of_week\"\n",
    "# ]\n",
    "\n",
    "# # 스테이지를 담는 배열\n",
    "# stages = []\n",
    "\n",
    "# # 범주형 변수에 대해 StringIndexer와 OneHotEncoder 적용\n",
    "# for c in cat_feats:\n",
    "#     cat_indexer = StringIndexer(inputCol=c, outputCol=c + \"_idx\").setHandleInvalid(\"keep\")\n",
    "#     onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c + \"_onehot\"])\n",
    "#     stages += [cat_indexer, onehot_encoder]\n",
    "\n",
    "# # 수치형 데이터에 대해 VectorAssembler와 StandardScaler 적용\n",
    "# from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# num_feats = [\n",
    "#     \"passenger_count\",\n",
    "#     \"trip_distance\",\n",
    "#     \"pickup_time\"\n",
    "# ]\n",
    "\n",
    "# # 수치형 변수를 벡터화하고 스케일링\n",
    "# for n in num_feats:\n",
    "#     num_assembler = VectorAssembler(inputCols=[n], outputCol=n + \"_vector\")  # 변경된 출력 컬럼 이름\n",
    "#     num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n + \"_scaled\")\n",
    "#     stages += [num_assembler, num_scaler]\n",
    "\n",
    "# # 범주형 특성과 수치형 특성을 결합하는 과정에서 출력 컬럼 이름을 변경\n",
    "# assembler_inputs = [c + \"_onehot\" for c in cat_feats] + [n + \"_scaled\" for n in num_feats]\n",
    "\n",
    "# # 결합된 특성 벡터를 저장할 새로운 출력 컬럼 이름 지정\n",
    "# assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"final_feature_vector\")  # 변경된 출력 컬럼 이름\n",
    "# stages += [assembler]\n",
    "\n",
    "# # 파이프라인 생성\n",
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "# pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# # 파이프라인 학습\n",
    "# fitted_transformer = pipeline.fit(train_df)\n",
    "\n",
    "# # 적용\n",
    "# vtrain_df = fitted_transformer.transform(train_df)\n",
    "\n",
    "# # 변환된 데이터프레임 출력\n",
    "# vtrain_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8ec44",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5736fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- passenger_count_vecotr: vector (nullable = true)\n",
      " |-- passenger_count_scaled: vector (nullable = true)\n",
      " |-- trip_distance_vecotr: vector (nullable = true)\n",
      " |-- trip_distance_scaled: vector (nullable = true)\n",
      " |-- pickup_time_vecotr: vector (nullable = true)\n",
      " |-- pickup_time_scaled: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver=\"normal\",\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"feature_vector\"\n",
    ")\n",
    "\n",
    "vtrain_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4042b601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- passenger_count_vecotr: vector (nullable = true)\n",
      " |-- passenger_count_scaled: vector (nullable = true)\n",
      " |-- trip_distance_vecotr: vector (nullable = true)\n",
      " |-- trip_distance_scaled: vector (nullable = true)\n",
      " |-- pickup_time_vecotr: vector (nullable = true)\n",
      " |-- pickup_time_scaled: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vtrain_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6282e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 17:13:19 WARN Instrumentation: [ce22825d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/12/18 17:17:06 WARN Instrumentation: [ce22825d] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = lr.fit(vtrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7da2f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, pickup_location_id: int, dropoff_location_id: int, trip_distance: double, pickup_time: int, day_of_week: string, total_amount: double, pickup_location_id_idx: double, pickup_location_id_onehot: vector, dropoff_location_id_idx: double, dropoff_location_id_onehot: vector, day_of_week_idx: double, day_of_week_onehot: vector, passenger_count_vecotr: vector, passenger_count_scaled: vector, trip_distance_vecotr: vector, trip_distance_scaled: vector, pickup_time_vecotr: vector, pickup_time_scaled: vector, feature_vector: vector, prediction: double]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vtest_df = fitted_transformer.transform(test_df)\n",
    "predictions = model.transform(vtest_df)\n",
    "# 캐싱을하여 나중에 쓰기 쉽게 만든다.\n",
    "predictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0cf5d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+------------------+\n",
      "|trip_distance|day_of_week|total_amount|        prediction|\n",
      "+-------------+-----------+------------+------------------+\n",
      "|          0.9|     Monday|         9.8|12.865737194280896|\n",
      "|          1.0|    Tuesday|       10.55|12.746845770078778|\n",
      "|          1.5|     Friday|        12.3| 16.22003685330334|\n",
      "|          5.1|   Thursday|        26.8|21.586481392813468|\n",
      "|          8.4|     Sunday|        32.8|28.625924538793235|\n",
      "|          2.2|    Tuesday|        13.3|13.316406587195926|\n",
      "|          0.7|  Wednesday|         5.8| 9.588440263875905|\n",
      "|          1.7|     Friday|         8.8|12.976060771403526|\n",
      "|         16.8|     Friday|       82.37| 71.78059156694991|\n",
      "|         14.2|  Wednesday|       85.65|  89.8114798292113|\n",
      "|          0.4|     Friday|         7.3|13.368775176364755|\n",
      "|          0.7|  Wednesday|        10.3|14.485899290783504|\n",
      "|          9.0|     Friday|        38.3|34.747976218143855|\n",
      "|          4.7|   Thursday|        21.3|23.633309383966058|\n",
      "|          1.4|   Thursday|       21.12|15.816210536497026|\n",
      "|          1.3|  Wednesday|       14.15| 15.50135266950496|\n",
      "|          2.6|  Wednesday|       17.25|17.442817661933752|\n",
      "|          4.3|  Wednesday|       21.05| 21.85171736835376|\n",
      "|          4.4|    Tuesday|        22.3|22.406181851611734|\n",
      "|          5.2|   Thursday|       24.35| 24.02588691089659|\n",
      "+-------------+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select([\"trip_distance\", \"day_of_week\", \"total_amount\", \"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bff52acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.8801773141961995"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ece5ffb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7966313304411026"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6cd8dc",
   "metadata": {},
   "source": [
    "### 스테이지 파이프라인 전처리를 활용해서 전보다 성능이 약 3% 올랐다\n",
    "아직 구현할게 많아서 다른 모델 학습이나, 하이퍼 파라미터 설정은 추후에 다시 진행할 예정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f054cdb",
   "metadata": {},
   "source": [
    " # 모델 저장 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "951f97db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 17:29:24 WARN Instrumentation: [94eb1f4a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/12/18 17:32:30 WARN Instrumentation: [94eb1f4a] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "[Stage 52:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|pickup_time|day_of_week|total_amount|pickup_location_id_idx|pickup_location_id_onehot|dropoff_location_id_idx|dropoff_location_id_onehot|day_of_week_idx|day_of_week_onehot|passenger_count_vecotr|passenger_count_scaled|trip_distance_vecotr|trip_distance_scaled|pickup_time_vecotr|  pickup_time_scaled|      feature_vector|        prediction|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|              0|                 4|                 79|          0.9|         14|     Monday|         9.8|                  62.0|         (263,[62],[1.0])|                   18.0|          (261,[18],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [0.9]|[0.23512390962789...|            [14.0]|[2.7343785789408197]|(534,[62,281,529,...|12.865737194280896|\n",
      "|              0|                 4|                107|          1.0|          9|    Tuesday|       10.55|                  62.0|         (263,[62],[1.0])|                   17.0|          (261,[17],[1.0])|            3.0|     (7,[3],[1.0])|                 [0.0]|                 [0.0]|               [1.0]|[0.26124878847543...|             [9.0]|  [1.75781480074767]|(534,[62,280,527,...|12.746845770078778|\n",
      "|              0|                 4|                148|          1.5|         21|     Friday|        12.3|                  62.0|         (263,[62],[1.0])|                   39.0|          (261,[39],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [1.5]|[0.39187318271315...|            [21.0]| [4.101567868411229]|(534,[62,302,524,...| 16.22003685330334|\n",
      "|              0|                 7|                 68|          5.1|          7|   Thursday|        26.8|                  63.0|         (263,[63],[1.0])|                   15.0|          (261,[15],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [5.1]|[1.3323688212247167]|             [7.0]|[1.3671892894704099]|(534,[63,278,525,...|21.586481392813468|\n",
      "|              0|                 7|                 82|          8.4|         21|     Sunday|        32.8|                  63.0|         (263,[63],[1.0])|                   93.0|          (261,[93],[1.0])|            6.0|     (7,[6],[1.0])|                 [0.0]|                 [0.0]|               [8.4]| [2.194489823193651]|            [21.0]| [4.101567868411229]|(534,[63,356,530,...|28.625924538793235|\n",
      "|              0|                 7|                129|          2.2|         13|    Tuesday|        13.3|                  63.0|         (263,[63],[1.0])|                   71.0|          (261,[71],[1.0])|            3.0|     (7,[3],[1.0])|                 [0.0]|                 [0.0]|               [2.2]|[0.5747473346459563]|            [13.0]|  [2.53906582330219]|(534,[63,334,527,...|13.316406587195926|\n",
      "|              0|                 7|                146|          0.7|         11|  Wednesday|         5.8|                  63.0|         (263,[63],[1.0])|                   86.0|          (261,[86],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [0.7]|[0.18287415193280...|            [11.0]|  [2.14844031202493]|(534,[63,349,526,...| 9.588440263875905|\n",
      "|              0|                 7|                226|          1.7|         13|     Friday|         8.8|                  63.0|         (263,[63],[1.0])|                   67.0|          (261,[67],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [1.7]|[0.4441229404082389]|            [13.0]|  [2.53906582330219]|(534,[63,330,524,...|12.976060771403526|\n",
      "|              0|                10|                140|         16.8|         18|     Friday|       82.37|                  84.0|         (263,[84],[1.0])|                   11.0|          (261,[11],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|              [16.8]| [4.388979646387302]|            [18.0]|  [3.51562960149534]|(534,[84,274,524,...| 71.78059156694991|\n",
      "|              0|                13|                  1|         14.2|         11|  Wednesday|       85.65|                  47.0|         (263,[47],[1.0])|                   84.0|          (261,[84],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|              [14.2]| [3.709732796351172]|            [11.0]|  [2.14844031202493]|(534,[47,347,526,...|  89.8114798292113|\n",
      "|              0|                13|                 12|          0.4|         15|     Friday|         7.3|                  47.0|         (263,[47],[1.0])|                  113.0|         (261,[113],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [0.4]|[0.10449951539017...|            [15.0]|  [2.92969133457945]|(534,[47,376,524,...|13.368775176364755|\n",
      "|              0|                13|                 13|          0.7|         16|  Wednesday|        10.3|                  47.0|         (263,[47],[1.0])|                   44.0|          (261,[44],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [0.7]|[0.18287415193280...|            [16.0]|  [3.12500409021808]|(534,[47,307,526,...|14.485899290783504|\n",
      "|              0|                13|                 42|          9.0|         17|     Friday|        38.3|                  47.0|         (263,[47],[1.0])|                   43.0|          (261,[43],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [9.0]|[2.3512390962789116]|            [17.0]|[3.3203168458567096]|(534,[47,306,524,...|34.747976218143855|\n",
      "|              0|                13|                 48|          4.7|         20|   Thursday|        21.3|                  47.0|         (263,[47],[1.0])|                   10.0|          (261,[10],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [4.7]| [1.227869305834543]|            [20.0]|   [3.9062551127726]|(534,[47,273,525,...|23.633309383966058|\n",
      "|              0|                13|                 87|          1.4|          8|   Thursday|       21.12|                  47.0|         (263,[47],[1.0])|                   46.0|          (261,[46],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [1.4]|[0.36574830386560...|             [8.0]|  [1.56250204510904]|(534,[47,309,525,...|15.816210536497026|\n",
      "|              0|                13|                 88|          1.3|         20|  Wednesday|       14.15|                  47.0|         (263,[47],[1.0])|                   58.0|          (261,[58],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [1.3]|[0.33962342501806...|            [20.0]|   [3.9062551127726]|(534,[47,321,526,...| 15.50135266950496|\n",
      "|              0|                13|                 90|          2.6|         10|  Wednesday|       17.25|                  47.0|         (263,[47],[1.0])|                   28.0|          (261,[28],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [2.6]|[0.6792468500361301]|            [10.0]|   [1.9531275563863]|(534,[47,291,526,...|17.442817661933752|\n",
      "|              0|                13|                100|          4.3|          6|  Wednesday|       21.05|                  47.0|         (263,[47],[1.0])|                   30.0|          (261,[30],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|               [4.3]| [1.123369790444369]|             [6.0]|[1.1718765338317798]|(534,[47,293,526,...| 21.85171736835376|\n",
      "|              0|                13|                100|          4.4|         15|    Tuesday|        22.3|                  47.0|         (263,[47],[1.0])|                   30.0|          (261,[30],[1.0])|            3.0|     (7,[3],[1.0])|                 [0.0]|                 [0.0]|               [4.4]|[1.1494946692919126]|            [15.0]|  [2.92969133457945]|(534,[47,293,527,...|22.406181851611734|\n",
      "|              0|                13|                137|          5.2|          7|   Thursday|       24.35|                  47.0|         (263,[47],[1.0])|                   26.0|          (261,[26],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [5.2]|[1.3584937000722601]|             [7.0]|[1.3671892894704099]|(534,[47,289,525,...| 24.02588691089659|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver=\"normal\",\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"feature_vector\"\n",
    ")\n",
    "\n",
    "# 훈련된 모델 학습\n",
    "model = lr.fit(vtrain_df)\n",
    "\n",
    "# 모델 저장\n",
    "model_dir = \"/home/lab01/src/project/taxi/model/\"\n",
    "\n",
    "# 이미 존재하는 모델이 있으면 덮어쓰고 저장\n",
    "model.write().overwrite().save(model_dir)\n",
    "\n",
    "# 모델 불러오기\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "lr_model = LinearRegressionModel.load(model_dir)  # 저장된 모델 불러오기\n",
    "\n",
    "# 테스트 데이터에 대해 예측\n",
    "vtest_df = fitted_transformer.transform(test_df)\n",
    "predictions = lr_model.transform(vtest_df)\n",
    "\n",
    "# 예측 결과 출력\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5114d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5.8801773141961995\n",
      "R2: 0.7966313304411026\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가 (RMSE, R²)\n",
    "print(\"RMSE:\", model.summary.rootMeanSquaredError)\n",
    "print(\"R2:\", model.summary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14e692ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd11120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd765720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb56f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f43299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da828aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad90a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aeb783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c7dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2730f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373d56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34edf0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e038d53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3317a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179efaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558e51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e59b9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879dcc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043f7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf6a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a55176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106e506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6385e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaaf3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c099c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac2b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdd435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark)",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
